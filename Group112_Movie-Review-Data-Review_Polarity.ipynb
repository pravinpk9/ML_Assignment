{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37f5472",
   "metadata": {},
   "source": [
    "### Group Details\n",
    "\n",
    "Group ID - 112\n",
    "Dataset Name - Movie Review - (Data-Review_Polarity)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca73b6",
   "metadata": {},
   "source": [
    "### Import required libraries and ignore the warning messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56c179ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If missing, then install the required libabries here.\n",
    "\n",
    "# pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cad4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile \n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem.wordnet import WordNetLemmatizer       #lemmatization\n",
    "from nltk.corpus.reader import NOUN, VERB, ADJ, ADV\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string #punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import os #read documents\n",
    "import re #url\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB             #Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import metrics                            #Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn.ensemble import RandomForestClassifier    #RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier #GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79544cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\P10506243\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download stopwords and use English stopwords as we have reviews in english language.\n",
    "nltk.download('stopwords')\n",
    "stop_en = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "380eeec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets of punctuation\n",
    "punctuation_translator = str.maketrans(\"\",\"\",string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94722899",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "617294a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open file \n",
    "#import tarfile \n",
    "#file = tarfile.open('review_polarity.tar.gz') \n",
    "# extracting file \n",
    "#file.extractall('./Extracted Data') \n",
    "#file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca940c9",
   "metadata": {},
   "source": [
    "### Read and clean the data\n",
    "\n",
    "Apply Tokenization (It will split the review data into single word).\n",
    "Remove single character, punctuation, stop words and web links\n",
    "Apply lemmitization (The process of converting a word to its base form)\n",
    "Apply POS Tag (POS tagger is used to assign grammatical information of each word of the sentence.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13eda75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_document_and_remove_messy_text(path, stop_words, punctuation_translator):\n",
    "    file = open(path, 'rb');\n",
    "    content = file.read()\n",
    "    words = word_tokenize(str(content))\n",
    "    \n",
    "    words_without_punctuation = []  \n",
    "    for word in words:\n",
    "        word = word.translate(punctuation_translator)\n",
    "        if len(word)>2:  #don't add empty strings or irrelevant one\n",
    "            words_without_punctuation.append(word)\n",
    "    #print(words_without_punctuation)\n",
    "    \n",
    "    words_without_stop_words_and_punctuation = [word for word in words_without_punctuation if not word in stop_en]\n",
    "    #print(words_without_stop_words_and_punctuation)\n",
    "    \n",
    "    words_without_stop_words_punctuation_and_url = []\n",
    "    for word in words_without_stop_words_and_punctuation:\n",
    "        word = re.sub(r\"http\\S+\", \"\", word)\n",
    "        words_without_stop_words_punctuation_and_url.append(word)\n",
    "    \n",
    "    # Lemmatization is the process of converting a word to its base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['N'] = wordnet.NOUN\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV\n",
    "    \n",
    "    # POS Tagging in NLTK is a process to mark up the words in text format for a particular part of a speech based on its definition and context\n",
    "    tags = nltk.pos_tag(words_without_stop_words_punctuation_and_url)\n",
    "    lemmas = [lemmatizer.lemmatize(token,tag_map[tag[0]]) for token,tag in tags]\n",
    "    \n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54bbd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read each and every file, and pass the review data for cleansing purpose.\n",
    "\n",
    "def clean_all_documents(folder_path, stop_words, punctuation_translator):\n",
    "    file_names = os.listdir(folder_path)\n",
    "    documents = [read_document_and_remove_messy_text(folder_path+'/'+file_name, stop_words, punctuation_translator) \n",
    "                 for file_name in file_names]\n",
    "    return documents\n",
    "\n",
    "negative_docs = clean_all_documents(\"Extracted Data/txt_sentoken/neg\",stop_en, punctuation_translator)\n",
    "positive_docs = clean_all_documents(\"Extracted Data/txt_sentoken/pos\",stop_en, punctuation_translator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4049d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list for all consolidated positive and negative reviews\n",
    "all_documents_string_array = []\n",
    "for i in range(len(negative_docs)):\n",
    "    sentence = ' '.join(negative_docs[i])\n",
    "    all_documents_string_array.append(sentence)\n",
    "for i in range(len(positive_docs)):\n",
    "    sentence = ' '.join(positive_docs[i])\n",
    "    all_documents_string_array.append(sentence)\n",
    "\n",
    "    \n",
    "# Check the count of reviews.\n",
    "len(all_documents_string_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447faaa0",
   "metadata": {},
   "source": [
    "### Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\n",
    "This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca90ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We have kept 5000 as max feature value. which will consider top 5000 words from entire dataset.\n",
    "# Consider the top max_features ordered by term frequency across the corpus.\n",
    "vectorizer = CountVectorizer(max_features = 5000, binary=True)   \n",
    "def vectorize_occurences(corpus, vectorizer):\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return X\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d166e6e5",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "We have used TfidfTransformer.\n",
    "\n",
    "Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.\n",
    "\n",
    "The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c496013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform a count matrix to a normalized tf or tf-idf representation\n",
    "# tf(t, d) is the number of times a term occurs in the given document.\n",
    "def calculate_frequencies(corpus):\n",
    "    tf_transformer = TfidfTransformer(use_idf=False).fit(corpus)\n",
    "    X = tf_transformer.transform(corpus)\n",
    "    return X\n",
    "\n",
    "# Create bag of words for all review words which we had cleaning in previous step\n",
    "bag_of_words_occurences = vectorize_occurences(all_documents_string_array, vectorizer)\n",
    "bag_of_words_frequencies = calculate_frequencies(bag_of_words_occurences)\n",
    "# print(bag_of_words_frequencies) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6764d8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of class values for positive and negative review data.\n",
    "#1000 0 for negatives and 1000 1 for positives\n",
    "\n",
    "number_of_documents_for_each_class = int(len(negative_docs))\n",
    "negative_labels = np.zeros((1,number_of_documents_for_each_class), dtype=int)[0]\n",
    "number_of_documents_for_each_class = int(len(positive_docs))\n",
    "positive_labels = np.ones((1,number_of_documents_for_each_class), dtype=int)[0]\n",
    "\n",
    "\n",
    "# Assign bag of words for negative and positive \n",
    "negative_bow = bag_of_words_frequencies[:number_of_documents_for_each_class]\n",
    "positive_bow = bag_of_words_frequencies[number_of_documents_for_each_class:]\n",
    "\n",
    "\n",
    "print('Negative Bag of words')\n",
    "print(negative_bow)\n",
    "\n",
    "print('\\nPositive Bag of words')\n",
    "print(positive_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9010304a",
   "metadata": {},
   "source": [
    "### Dataset details:\n",
    "Data consist of 1000 positive reviews with class value = 1 and 1000 reviews of negative reviews which we have marked 0 as class value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275671ee",
   "metadata": {},
   "source": [
    "### Multinomial Calssifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477c1a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a Multinomial Naive Bayes Classifier\n",
    "gnb = MultinomialNB()           #good\n",
    "def multinomial_calssifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):\n",
    "    #Train the model using the training sets\n",
    "    gnb.fit(features_training_set, labels_training_set)\n",
    "    #Predict the response for test dataset\n",
    "    pred = gnb.predict(features_test_set)\n",
    "    # Model Accuracy, how often is the classifier correct?\n",
    "    print(\"Multinomial Naive Bayes Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('multinomial_calssifier '+ str(fold)+'.xlsx')\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d63b408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "    \n",
    "    average_accuracy = multinomial_calssifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "\n",
    "print('---------- Completed Multinomial Calssifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca39428",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e38dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):   #best\n",
    "    clf = RandomForestClassifier(n_estimators = 1000)\n",
    "    #rfe = RFE(estimator=clf, step=0.5)\n",
    "    clf = clf.fit(features_training_set, labels_training_set)     #maybe use rfe instean of clf??\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Random Forest Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('random_forest_classifier '+ str(fold)+'.xlsx')\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8ae044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):    # \n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "    \n",
    "    average_accuracy = random_forest_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    \n",
    "print('---------- Completed Random Forest Classifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8440cc3",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):  #asa si asa\n",
    "    clf = GradientBoostingClassifier(n_estimators=100)\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Gradient Boosting Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('gradient_boosting_classifier '+ str(fold)+'.xlsx')\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2aa5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "    \n",
    "    average_accuracy = gradient_boosting_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    \n",
    "    print('---------- Completed Gradient Boosting Classifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c040902",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29785892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):\n",
    "    clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial')\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Logistic Regression Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "        # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('logistic_regression_classifier '+ str(fold)+'.xlsx')\n",
    "    \n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "    \n",
    "    average_accuracy = logistic_regression_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold) \n",
    "    \n",
    "print('---------- Completed Logistic Regression Classifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba228a98",
   "metadata": {},
   "source": [
    "### SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4458da02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold):  #best\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"SVM Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "        # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('svm_classifier '+ str(fold)+'.xlsx')\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6477b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "\n",
    "    average_accuracy = svm_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "    \n",
    "print('---------- Completed SVM Classifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6386c55",
   "metadata": {},
   "source": [
    "### SVM Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad51c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_linear_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy, fold): #good\n",
    "    clf = svm.LinearSVC()\n",
    "    clf = clf.fit(features_training_set, labels_training_set)\n",
    "    pred = clf.predict(features_test_set)\n",
    "    print(\"Linear SVM Classifier Accuracy:\",metrics.accuracy_score(labels_test_set, pred))\n",
    "    average_accuracy += metrics.accuracy_score(labels_test_set, pred)\n",
    "    print(\"Average accuracy: \", average_accuracy/fold)\n",
    "    print(\"\")\n",
    "    print(metrics.classification_report(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(metrics.confusion_matrix(labels_test_set, pred))\n",
    "    print(\"\")\n",
    "        # Convert class label lists to dictionary \n",
    "    dict = {'Actual': labels_test_set, 'Predicted': pred} \n",
    "    df = pd.DataFrame(dict)\n",
    "    # Save data to excel\n",
    "    df.to_excel('svm_linear_classifier '+ str(fold)+'.xlsx')\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2dc39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use cross validation for greater accuracy\n",
    "kfold = KFold(n_splits = 5)  \n",
    "fold = 0; average_accuracy = 0;\n",
    "\n",
    "for train, test in kfold.split(negative_bow):\n",
    "    fold = fold+1\n",
    "    features_training_set = negative_bow[train].toarray().tolist()  # get training data\n",
    "    features_training_set += positive_bow[train].toarray().tolist() # get training data\n",
    "\n",
    "    features_test_set = negative_bow[test].toarray().tolist()       # get testing data\n",
    "    features_test_set += positive_bow[test].toarray().tolist()      # get testing data\n",
    "    \n",
    "    labels_training_set = negative_labels[train].tolist()           # get training label(class) data\n",
    "    labels_training_set += positive_labels[train].tolist()          # get training label(class) data\n",
    "    \n",
    "    labels_test_set = negative_labels[test].tolist()                # get testing label(class) data\n",
    "    labels_test_set += positive_labels[test].tolist()               # get testing label(class) data\n",
    "    \n",
    "    average_accuracy = svm_linear_classifier(features_training_set, labels_training_set, features_test_set, labels_test_set, average_accuracy ,fold)\n",
    "\n",
    "print('---------- Completed SVM Linear Classifier --------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6d7da",
   "metadata": {},
   "source": [
    "### Create Output Data\n",
    "\n",
    "From the above different modules, we can say that Multinomial Calssifier is the best module which shows accuracy till 86%. Lets create the output file using the same module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc748a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
